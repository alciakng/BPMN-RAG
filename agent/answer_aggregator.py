# answer_aggregator.py
"""
ReAct Framework Step 3: Answer Aggregation
Stage-1 ë‹µë³€ê³¼ ì™¸ë¶€ ì§€ì‹ì„ ê²°í•©í•˜ì—¬ ì„¹ì…˜ ë¶„ë¦¬ëœ ìµœì¢… ë‹µë³€ ìƒì„±
"""
from __future__ import annotations

from typing import Any, Dict, Optional

from common.logger import Logger
from common.llm_client import LLMClient
from manager.util import _extract_text

LOGGER = Logger.get_logger("agent.answer_aggregator")


class AnswerAggregator:
    """
    Aggregate Stage-1 answer with external knowledge insights

    Responsibilities:
    - Combine GraphDB-based answer (Stage-1) with external knowledge (Stage-2)
    - Generate section-separated final answer using LLM
    - Ensure clear attribution and source citation
    - Handle cases where no external knowledge is available
    """

    def __init__(self, llm_client: LLMClient):
        """
        Args:
            llm_client: LLM client for answer aggregation
        """
        if not llm_client:
            raise ValueError("LLM client is required for AnswerAggregator")

        self.llm = llm_client

    def aggregate(
        self,
        user_query: str,
        stage1_answer: str,
        augmentation_result: Dict[str, Any],
        intent_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Aggregate Stage-1 answer with external knowledge

        Args:
            user_query: Original user query
            stage1_answer: Stage-1 answer from GraphDB
            augmentation_result: External knowledge from KnowledgeAugmenter
            intent_result: Intent analysis result

        Returns:
            {
                "final_answer": str,              # Complete answer (Korean)
                "has_stage2": bool,               # Stage-2 insights included?
                "sections": {
                    "stage1": str,                # GraphDB-based answer
                    "stage2": Optional[str]       # External knowledge insights
                },
                "sources": List[Dict],            # External sources cited
                "metadata": {
                    "total_sources": int,
                    "generation_method": str      # "stage1_only" | "2stage_aggregated"
                }
            }
        """
        try:
            LOGGER.info("[AGGREGATE] Starting answer aggregation")
            LOGGER.info("[AGGREGATE] has_insights=%s, total_sources=%d",
                       augmentation_result.get("has_insights"),
                       augmentation_result.get("total_sources", 0))

            # Case 1: No external knowledge - return Stage-1 only
            if not augmentation_result.get("has_insights"):
                LOGGER.info("[AGGREGATE] No external insights, returning Stage-1 only")
                return self._stage1_only_result(
                    user_query=user_query,
                    stage1_answer=stage1_answer,
                    reason="No external knowledge found or not needed"
                )

            # Case 2: Aggregate Stage-1 + Stage-2
            LOGGER.info("[AGGREGATE] Aggregating Stage-1 + Stage-2")
            final_answer = self._llm_aggregate(
                user_query=user_query,
                stage1_answer=stage1_answer,
                augmentation_result=augmentation_result,
                intent_result=intent_result
            )

            if not final_answer:
                # Fallback if LLM aggregation fails
                LOGGER.warning("[AGGREGATE] LLM aggregation failed, using fallback format")
                final_answer = self._fallback_aggregate(
                    stage1_answer=stage1_answer,
                    augmentation_result=augmentation_result
                )

            # Extract sources
            sources = self._extract_sources(augmentation_result)

            result = {
                "final_answer": final_answer,
                "has_stage2": True,
                "sections": {
                    "stage1": stage1_answer,
                    "stage2": augmentation_result.get("context_summary", "")
                },
                "sources": sources,
                "metadata": {
                    "total_sources": augmentation_result.get("total_sources", 0),
                    "generation_method": "2stage_aggregated"
                }
            }

            LOGGER.info("[AGGREGATE] Complete - final_answer_len=%d, sources=%d",
                       len(final_answer), len(sources))

            return result

        except Exception as e:
            LOGGER.exception("[AGGREGATE][ERROR] Aggregation failed: %s", e)
            # Fallback to Stage-1 only
            return self._stage1_only_result(
                user_query=user_query,
                stage1_answer=stage1_answer,
                reason=f"Aggregation error: {str(e)}"
            )

    # ============================================================
    # LLM-based Aggregation
    # ============================================================

    def _llm_aggregate(
        self,
        user_query: str,
        stage1_answer: str,
        augmentation_result: Dict[str, Any],
        intent_result: Dict[str, Any]
    ) -> Optional[str]:
        """
        Use LLM to generate section-separated final answer

        Returns:
            Final answer in Korean with clear section separation
        """
        try:
            # Build external knowledge context text
            external_knowledge_text = self._format_external_knowledge(augmentation_result)

            system_prompt = """
[ROLE]
You are a BPMN Process Intelligence expert providing comprehensive answers.

[TASK]
Combine the GraphDB-based answer (Stage-1) with external knowledge insights (Stage-2) to provide a complete, actionable answer.

[OUTPUT FORMAT]
Generate a well-structured Korean answer with TWO clear sections:

---
##  í”„ë¡œì„¸ìŠ¤ ë¶„ì„ ê²°ê³¼

[Stage-1 answer content - preserve original format and structure]
- DO NOT summarize or paraphrase the Stage-1 answer
- Keep all tables, bullet points, and formatting as-is
- Only compress if the content is excessively long (>2000 characters):
  - Remove redundant explanations
  - Condense verbose descriptions while keeping key points
  - Maintain all critical data (IDs, metrics, evidence)

---
## ê°œì„  ì œì•ˆ ë° ì°¸ê³  ì‚¬í•­

[Synthesize external knowledge into a consulting-style insight report]

### Executive Summary
- **í•µì‹¬ ë°œê²¬ì‚¬í•­**: [1-2ë¬¸ìž¥ìœ¼ë¡œ ì™¸ë¶€ ì§€ì‹ì—ì„œ ë°œê²¬ëœ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìš”ì•½]
- **ë¹„ì¦ˆë‹ˆìŠ¤ ìž„íŒ©íŠ¸**: [í˜„ìž¬ í”„ë¡œì„¸ìŠ¤ ëŒ€ë¹„ ê°œì„  ì‹œ ê¸°ëŒ€íš¨ê³¼ë¥¼ ì •ëŸ‰ì /ì •ì„±ì ìœ¼ë¡œ ì œì‹œ]

### ì „ëžµì  ê°œì„  ë°©í–¥

#### 1. [ê°œì„  ì˜ì—­ 1 - ì˜ˆ: í”„ë¡œì„¸ìŠ¤ ìžë™í™” í™•ëŒ€]
- **í˜„í™©**: [í˜„ìž¬ GraphDB ë¶„ì„ ê²°ê³¼ì™€ ì—°ê³„í•œ ë¬¸ì œì ]
- **Best Practice**: [ì™¸ë¶€ ì§€ì‹ ê¸°ë°˜ ëª¨ë²” ì‚¬ë¡€ - êµ¬ì²´ì  ì‚¬ë¡€/ìˆ˜ì¹˜ í¬í•¨]
- **ê¶Œìž¥ ì‚¬í•­**: [ì‹¤í–‰ ê°€ëŠ¥í•œ 3-5ê°œì˜ êµ¬ì²´ì  ì•¡ì…˜ ì•„ì´í…œ]
- **êµ¬í˜„ ë‚œì´ë„**: [ìƒ/ì¤‘/í•˜] | **ì˜ˆìƒ íš¨ê³¼**: [ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ]

#### 2. [ê°œì„  ì˜ì—­ 2 - ì˜ˆ: ë¦¬ìŠ¤í¬ ê´€ë¦¬ ê°•í™”]
- **í˜„í™©**: [í˜„ìž¬ GraphDB ë¶„ì„ ê²°ê³¼ì™€ ì—°ê³„í•œ ë¬¸ì œì ]
- **Best Practice**: [ì™¸ë¶€ ì§€ì‹ ê¸°ë°˜ ëª¨ë²” ì‚¬ë¡€]
- **ê¶Œìž¥ ì‚¬í•­**: [ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ ì•„ì´í…œ]
- **êµ¬í˜„ ë‚œì´ë„**: [ìƒ/ì¤‘/í•˜] | **ì˜ˆìƒ íš¨ê³¼**: [ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ]

[í•„ìš”ì‹œ ê°œì„  ì˜ì—­ 3, 4 ì¶”ê°€...]

### ë¦¬ìŠ¤í¬ ë° ê³ ë ¤ì‚¬í•­
- [ëŒ€ì•ˆ ë„ìž… ì‹œ ë°œìƒ ê°€ëŠ¥í•œ ë¦¬ìŠ¤í¬ 1 - ì™„í™” ë°©ì•ˆ í¬í•¨]
- [ëŒ€ì•ˆ ë„ìž… ì‹œ ë°œìƒ ê°€ëŠ¥í•œ ë¦¬ìŠ¤í¬ 2 - ì™„í™” ë°©ì•ˆ í¬í•¨]
- [ì¡°ì§/ê¸°ìˆ ì  ì œì•½ì‚¬í•­ - ì‹¤í–‰ ì‹œ ì£¼ì˜ì‚¬í•­]

### ë²¤ì¹˜ë§ˆí¬ ë° ì‚°ì—… í‘œì¤€
- [ê´€ë ¨ ì‚°ì—…/ë„ë©”ì¸ì˜ í‘œì¤€ í”„ë¡œì„¸ìŠ¤ ë ˆí¼ëŸ°ìŠ¤ - APQC, ISO ë“±]
- [ì£¼ìš” ê¸°ì—… ì‚¬ë¡€ - AWS/Azure/GCP Well-Architected Framework ê¸°ë°˜]
- [ì •ëŸ‰ì  ë²¤ì¹˜ë§ˆí¬ ìˆ˜ì¹˜ - ê°€ëŠ¥í•œ ê²½ìš°]

### ðŸ”— ì°¸ê³  ìžë£Œ ë° ì¶œì²˜
1. **[Source 1 Title]** - [í•µì‹¬ ë‚´ìš© 1-2ë¬¸ìž¥ ìš”ì•½]
   ðŸ”— [URL]
2. **[Source 2 Title]** - [í•µì‹¬ ë‚´ìš© 1-2ë¬¸ìž¥ ìš”ì•½]
   ðŸ”— [URL]

---

[REQUIREMENTS]
1. **Preserve Stage-1 Format**: Keep original markdown structure (tables, bullets, code blocks, bold text)
2. **Section Separation**: Clearly separate Stage-1 and Stage-2 with markdown headers
3. **Consulting-Style Insights**: Write "ê°œì„  ì œì•ˆ ë° ì°¸ê³  ì‚¬í•­" as a professional consulting report with:
   - Executive Summary (í•µì‹¬ ë°œê²¬ì‚¬í•­ + ë¹„ì¦ˆë‹ˆìŠ¤ ìž„íŒ©íŠ¸)
   - Strategic Improvement Areas (ì „ëžµì  ê°œì„  ë°©í–¥) - organized by themes
   - Risk Assessment (ë¦¬ìŠ¤í¬ ë° ê³ ë ¤ì‚¬í•­)
   - Benchmarks & Industry Standards (ë²¤ì¹˜ë§ˆí¬ ë° ì‚°ì—… í‘œì¤€)
4. **Evidence-Based**: Link Stage-1 findings to external knowledge insights
5. **Quantitative When Possible**: Include metrics, ROI estimates, benchmark numbers
6. **Actionable & Prioritized**: Provide specific action items with difficulty/impact ratings
7. **Source Attribution**: Cite external sources with brief summaries and URLs
8. **Korean**: All content must be in Korean
9. **Synthesis Over Listing**: Integrate external knowledge into coherent strategic narrative

[IMPORTANT]
- DO NOT rewrite or summarize Stage-1 unless it exceeds 2000 characters
- **Connect Stage-1 to Stage-2**: Explicitly link GraphDB findings to external insights in "í˜„í™©" field
- **Be Specific**: Replace generic advice with concrete examples from sources (e.g., "AWS recommends X pattern for Y scenario")
- **Assess Trade-offs**: For each recommendation, mention implementation complexity and expected impact
- If external knowledge contradicts Stage-1, present both perspectives and recommend reconciliation approach
- Prioritize tier-1 enterprise sources (AWS, Azure, GCP, APQC) over generic content
- Structure improvements by strategic themes (automation, risk management, cost optimization, etc.)
- Include industry benchmarks when available to contextualize current performance
"""

            user_message = f"""
[USER QUERY]
{user_query}

[STAGE-1 ANSWER]
{stage1_answer}

{external_knowledge_text}

[INTENT ANALYSIS]
- Needs Insight: {intent_result.get('needs_insight', False)}
- Confidence: {intent_result.get('confidence', 0.0):.2f}

[TASK]
Generate the final aggregated answer by combining Stage-1 answer with external knowledge insights.
"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ]

            LOGGER.info("[AGGREGATE][LLM] Calling LLM for aggregation")
            raw_response = self.llm.complete(messages)
            final_answer = _extract_text(raw_response)

            LOGGER.info("[AGGREGATE][LLM] Generated answer, length=%d", len(final_answer))

            return final_answer

        except Exception as e:
            LOGGER.exception("[AGGREGATE][LLM][ERROR] %s", e)
            return None

    # ============================================================
    # Fallback Aggregation (Template-based)
    # ============================================================

    def _fallback_aggregate(
        self,
        stage1_answer: str,
        augmentation_result: Dict[str, Any]
    ) -> str:
        """
        Template-based aggregation when LLM fails

        Returns:
            Simple concatenation with section headers
        """
        LOGGER.info("[AGGREGATE][FALLBACK] Using template-based aggregation")

        final_answer = "---\n## ðŸ“Š í”„ë¡œì„¸ìŠ¤ ë¶„ì„ ê²°ê³¼ (GraphDB ê¸°ë°˜)\n\n"
        final_answer += stage1_answer
        final_answer += "\n\n---\n## ðŸ’¡ ê°œì„  ì œì•ˆ ë° ì°¸ê³  ì‚¬í•­ (ì™¸ë¶€ ì§€ì‹ ê¸°ë°˜)\n\n"

        insights_by_aspect = augmentation_result.get("insights_by_aspect", {})

        aspect_korean = {
            "background": "ë°°ê²½ ì§€ì‹",
            "alternatives": "ëŒ€ì•ˆ ì„¤ê³„",
            "best_practices": "ëª¨ë²” ì‚¬ë¡€",
            "risks": "ë¦¬ìŠ¤í¬ ë¶„ì„",
            "quantitative": "ì •ëŸ‰ì  ì§€í‘œ"
        }

        if insights_by_aspect:
            for aspect, items in insights_by_aspect.items():
                aspect_name = aspect_korean.get(aspect, aspect)
                final_answer += f"### {aspect_name}\n\n"

                for idx, item in enumerate(items, 1):
                    content = item.get("content", "")
                    source = item.get("source", "Unknown")
                    url = item.get("url", "")

                    final_answer += f"**{idx}. {source}**\n"
                    final_answer += f"{content}\n"

                    if url:
                        final_answer += f"[ìƒì„¸ë³´ê¸°]({url})\n"

                    final_answer += "\n"
        else:
            final_answer += "ì™¸ë¶€ ì§€ì‹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n"

        final_answer += "\n---\n"

        return final_answer

    # ============================================================
    # Stage-1 Only Result
    # ============================================================

    def _stage1_only_result(
        self,
        user_query: str,
        stage1_answer: str,
        reason: str = ""
    ) -> Dict[str, Any]:
        """
        Return Stage-1 only result (no external knowledge)
        """
        LOGGER.info("[AGGREGATE] Returning Stage-1 only result: %s", reason)

        return {
            "final_answer": stage1_answer,
            "has_stage2": False,
            "sections": {
                "stage1": stage1_answer,
                "stage2": None
            },
            "sources": [],
            "metadata": {
                "total_sources": 0,
                "generation_method": "stage1_only",
                "reason": reason
            }
        }

    # ============================================================
    # Utilities
    # ============================================================

    def _format_external_knowledge(self, augmentation_result: Dict[str, Any]) -> str:
        """
        Format augmentation result as text for LLM consumption

        Returns:
            Formatted text describing external knowledge by aspect
        """
        insights_by_aspect = augmentation_result.get("insights_by_aspect", {})

        if not insights_by_aspect:
            return "[EXTERNAL KNOWLEDGE]\nNo external knowledge found."

        lines = ["[EXTERNAL KNOWLEDGE]"]

        for category, items in insights_by_aspect.items():
            lines.append(f"\n## External Knowledge\n")

            for idx, item in enumerate(items, 1):
                content = item.get("content", "")
                source = item.get("source", "Unknown")
                url = item.get("url", "")
                tier = item.get("tier", "unknown")

                lines.append(f"{idx}. [{tier.upper()}] {source}")
                lines.append(f"   {content[:200]}...")
                if url:
                    lines.append(f"   URL: {url}")
                lines.append("")

        return "\n".join(lines)

    def _extract_sources(self, augmentation_result: Dict[str, Any]) -> list[Dict[str, str]]:
        """
        Extract unique sources from augmentation result

        Returns:
            [
                {
                    "title": str,
                    "url": str,
                    "domain": str,  # "aws" | "azure" | "gcp" | "apqc"
                    "tier": str     # "broad" | "focused"
                }
            ]
        """
        sources = []
        seen_urls = set()

        insights_by_aspect = augmentation_result.get("insights_by_aspect", {})

        for category, items in insights_by_aspect.items():
            for item in items:
                url = item.get("url", "")

                # Skip duplicates
                if url in seen_urls:
                    continue

                seen_urls.add(url)

                sources.append({
                    "title": item.get("source", "Unknown"),
                    "url": url,
                    "domain": item.get("metadata", {}).get("domain_source", "unknown"),
                    "tier": item.get("tier", "unknown"),
                    "category": category
                })

        LOGGER.info("[EXTRACT_SOURCES] Extracted %d unique sources", len(sources))
        return sources
